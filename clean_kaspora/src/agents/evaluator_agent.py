"""
Evaluator Agent - Validates insights for quality and confidence

This agent scores and validates insights generated by the Insight Agent.
"""
import json
from typing import Dict, Any
import google.generativeai as genai
import os
from dotenv import load_dotenv
from ..utils.prompt_manager import PromptManager
from ..utils import convert_to_serializable

load_dotenv()


class EvaluatorAgent:
    """Evaluates and validates insights"""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize the evaluator agent"""
        genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
        self.model = genai.GenerativeModel(
            model_name=config.get('model', 'gemini-1.5-flash'),
            generation_config={
                'temperature': 0.3,  # Lower temperature for consistency
                'max_output_tokens': config.get('max_tokens', 2000),
            }
        )
        self.prompt_manager = PromptManager()
        self.confidence_threshold = config.get('confidence_min', 0.6)
    
    def evaluate_insight(self, insight: Dict[str, Any], data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate a single insight
        
        Args:
            insight: Insight to evaluate
            data: Supporting data
            
        Returns:
            Evaluation result with scores
        """
        # Convert numpy types and load prompt template
        serializable_insight = convert_to_serializable(insight)
        serializable_data = convert_to_serializable(data)
        
        prompt = self.prompt_manager.get_filled_prompt(
            'evaluator_agent',
            insight=json.dumps(serializable_insight, indent=2),
            data=json.dumps(serializable_data, indent=2),
            confidence_min=self.confidence_threshold
        )
        
        system_instruction = "You are a rigorous quality assurance analyst. Always return valid JSON."
        full_prompt = f"{system_instruction}\n\n{prompt}"
        
        try:
            response = self.model.generate_content(full_prompt)
            
            # Extract JSON from response
            response_text = response.text.strip()
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            evaluation = json.loads(response_text)
            
            # Determine if insight passed
            evaluation['passed'] = evaluation.get('overall_score', 0) >= self.confidence_threshold
            
            return evaluation
            
        except Exception as e:
            print(f"Error evaluating insight: {e}")
            return {
                'overall_score': 0.0,
                'passed': False,
                'scores': {
                    'evidence_quality': 0.0,
                    'statistical_validity': 0.0,
                    'actionability': 0.0,
                    'business_relevance': 0.0
                },
                'verdict': 'reject',
                'error': str(e)
            }


def evaluate_insights(config: Dict[str, Any], insights: list, data: Dict[str, Any]) -> list:
    """
    Evaluate multiple insights
    
    Args:
        config: Configuration dictionary
        insights: List of insights to evaluate
        data: Supporting data
        
    Returns:
        List of evaluated insights with scores
    """
    agent = EvaluatorAgent(config)
    evaluated = []
    
    for insight in insights:
        evaluation = agent.evaluate_insight(insight, data)
        evaluated.append({
            'insight': insight,
            'evaluation': evaluation,
            'passed': evaluation['passed']
        })
    
    return evaluated
